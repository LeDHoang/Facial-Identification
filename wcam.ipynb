{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is to recognize faces on live camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image,ImageDraw\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing MTCNN and InceptionResnetV1 \n",
    "\n",
    "mtcnn0 = MTCNN(image_size=240, margin=0, keep_all=False, min_face_size=40) # keep_all=False\n",
    "mtcnn = MTCNN(image_size=240, margin=0, keep_all=True, min_face_size=40) # keep_all=True\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from folder\n",
    "\n",
    "dataset = datasets.ImageFolder('poliface_origin') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn0(img, return_prob=True) \n",
    "    if face is not None and prob>0.92:\n",
    "        emb = resnet(face.unsqueeze(0)) \n",
    "        embedding_list.append(emb.detach()) \n",
    "        name_list.append(idx_to_class[idx])        \n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list] \n",
    "torch.save(data, 'data.pt') # saving data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADat_Nacentech 0.5483829379081726 index1\n",
      "ADat_Nacentech 0.5318140983581543 index2\n",
      "ADat_Nacentech 0.5164004564285278 index3\n",
      "ADat_Nacentech 0.5918645858764648 index4\n",
      "ADat_Nacentech 0.3137890100479126 index5\n",
      "ADat_Nacentech 0.20562462508678436 index6\n",
      "ADat_Nacentech 0.5330504179000854 index7\n",
      "ADat_Nacentech 0.5230152010917664 index8\n",
      "Duong_guest_NICexpo 0.5596486926078796 index9\n",
      "AHung_Nacentech 0.46456512808799744 index10\n",
      "AHung_Nacentech 0.4400879442691803 index11\n",
      "AHung_Nacentech 0.4041878581047058 index12\n",
      "AHung_Nacentech 0.43235644698143005 index13\n",
      "AHung_Nacentech 0.4629649519920349 index14\n",
      "AHung_Nacentech 0.47770097851753235 index15\n",
      "AHung_Nacentech 0.521297812461853 index16\n",
      "AHung_Nacentech 0.5397700071334839 index17\n",
      "AHung_Nacentech 0.23571129143238068 index18\n",
      "AHung_Nacentech 0.4477018117904663 index19\n",
      "AKet_guest_NICexpo 0.5590885877609253 index20\n",
      "AQuyet_guest_NICexpo 0.5464585423469543 index21\n",
      "AToan_guest_NICexpo 0.5687696933746338 index22\n",
      "AToan_guest_NICexpo 0.5723209977149963 index23\n",
      "AToan_guest_NICexpo 0.5755997896194458 index24\n",
      "BuiTienDai_guest_NICexpo 0.556242048740387 index25\n",
      "LeDucLong_NicExpo 0.5694119930267334 index26\n",
      "Trang_guest_NICexpo 0.5882213115692139 index27\n",
      "ChiChung 0.49771106243133545 index28\n",
      "ChiTrang_NicExpo 0.26969701051712036 index29\n",
      "HuuPhuong_guest_NICexpo 0.5766056180000305 index30\n",
      "Chuong 0.5419831871986389 index31\n",
      "Chuong 0.5469859838485718 index32\n",
      "CongLuong_guest_NICexpo 0.5820377469062805 index33\n",
      "DLien_guest_NICexpo 0.5361341238021851 index34\n",
      "DLien_guest_NICexpo 0.4512053430080414 index35\n",
      "DinhVanHuy_guest_NICexpo 0.5732273459434509 index36\n",
      "DinhVanHuy_guest_NICexpo 0.5765476226806641 index37\n",
      "DoDieuHang_NicExpo 0.5655672550201416 index38\n",
      "Duong_guest_NICexpo 0.5715709924697876 index39\n",
      "HaChi_guest_NICexpo 0.5051166415214539 index40\n",
      "HaChi_guest_NICexpo 0.552553653717041 index41\n",
      "Ha_SV_DHCN 0.5160353779792786 index42\n",
      "Ha_SV_DHCN 0.4889684319496155 index43\n",
      "Ha_SV_DHCN 0.43984392285346985 index44\n",
      "Ha_SV_DHCN 0.5512526035308838 index45\n",
      "Ha_SV_DHCN 0.5021880865097046 index46\n",
      "Ha_SV_DHCN 0.4549978971481323 index47\n",
      "Ha_SV_DHCN 0.5429539680480957 index48\n",
      "Ha_SV_DHCN 0.41507652401924133 index49\n",
      "HoaHMI_guest_NICexpo 0.4234471321105957 index50\n",
      "HoaHMI_guest_NICexpo 0.557349443435669 index51\n",
      "HoaHMI_guest_NICexpo 0.40259963274002075 index52\n",
      "HungKet_guest_NICexpo 0.53011155128479 index53\n",
      "HuuPhuong_guest_NICexpo 0.4980350434780121 index54\n",
      "DoDieuHang_NicExpo 0.5772642493247986 index55\n",
      "thanh tra 0.576171875 index56\n",
      "MaiPhuong_guest_NICexpo 0.4280478060245514 index57\n",
      "MaiPhuong_guest_NICexpo 0.4369766414165497 index58\n",
      "MaiPhuong_guest_NICexpo 0.4083080589771271 index59\n",
      "NguyenNgocTrung_NicExpo 0.4664619565010071 index60\n",
      "Ninh_guest_NICexpo 0.4982549250125885 index61\n",
      "PVVOV_guest_NICexpo 0.5332512259483337 index62\n",
      "PVVOV_guest_NICexpo 0.5616241097450256 index63\n",
      "PhamHuuTuyen_guest_NICexpo 0.5215431451797485 index64\n",
      "PhamHuuTuyen_guest_NICexpo 0.5090821981430054 index65\n",
      "Quang_NicExpo 0.42081218957901 index66\n",
      "Quang_NicExpo 0.5525158047676086 index67\n",
      "Seunghoon Sa 0.5406450629234314 index68\n",
      "Seunghoon Sa 0.5785626173019409 index69\n",
      "HoaHMI_guest_NICexpo 0.5137853026390076 index70\n",
      "Seunghoon Sa 0.3931843042373657 index71\n",
      "Seunghoon Sa 0.0 index72\n",
      "Seunghoon Sa 0.585984468460083 index73\n",
      "Seunghoon Sa 0.3700334131717682 index74\n",
      "Seunghoon Sa 0.5838742256164551 index75\n",
      "khanh linh_NicExpo 0.5469450354576111 index76\n",
      "Thang_light_all_134 0.47314393520355225 index77\n",
      "Thang_light_all_134 0.5122207999229431 index78\n",
      "Thang_light_all_134 0.5533575415611267 index79\n",
      "Thang_light_all_134 0.4895969331264496 index80\n",
      "Thang_light_all_134 0.5570756793022156 index81\n",
      "le thanh quang 0.5640826225280762 index82\n",
      "do mai lan_NicExpo 0.5525116324424744 index83\n",
      "hanh_NicExpo 0.5706353783607483 index84\n",
      "loan_NicExpo 0.5733403563499451 index85\n",
      "Loan_guest_NICexpo 0.5489859580993652 index86\n",
      "nguuen thi llu_NicExpo 0.42043668031692505 index87\n",
      "thanh tra 0.40896084904670715 index88\n",
      "trang_NicExpo 0.48765090107917786 index89\n",
      "yen_ NicExpo 0.5464859008789062 index90\n",
      "90 90\n",
      "Accuracy rate:\n",
      "0.8777777777777778\n"
     ]
    }
   ],
   "source": [
    "data_test = datasets.ImageFolder('poliface_test')\n",
    "idx_to_class_test={i:c for c,i in data_test.class_to_idx.items()}\n",
    "loader_test = DataLoader(data_test,collate_fn=collate_fn)\n",
    "count=1\n",
    "name_true=[]\n",
    "name_test=[]\n",
    "temp_name=''\n",
    "for img, idx in loader_test:\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    #print(img_cropped_list[0])\n",
    "    #print(idx_to_class_test[idx])\n",
    "    temp_name=idx_to_class_test[idx]\n",
    "    if img_cropped_list is not None:\n",
    "        \"\"\"boxes, _ = mtcnn.detect(img)\n",
    "        #print(prob_list)\n",
    "        img_array=np.asarray(img)\n",
    "        im_arr_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "        #temp_frame = img.copy()\n",
    "        prob_list_list = prob_list.tolist()\n",
    "        max_value = max(prob_list_list)\n",
    "\n",
    "        max_index = prob_list_list.index(max_value)\n",
    "        temp_name = idx_to_class_test\"\"\"\n",
    "        for i,prob in enumerate(prob_list):\n",
    "            if prob>0.60:\n",
    "                #temp_frame= embedding_list\n",
    "                #print('Probability of face:')\n",
    "                #print(prob)\n",
    "                \n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                    \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                    \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "        box = boxes[max_index]\n",
    "        c1,c2 = (int(np.float32(box[0])),int(np.float32(box[1]))),(int(np.float32(box[2])),int(np.float32(box[3])))\n",
    "        cv2.rectangle(im_arr_bgr,c1,c2,(255,0,0),5)\n",
    "        img_array=cv2.cvtColor(im_arr_bgr,cv2.COLOR_BGR2RGB)\n",
    "        im2 = Image.fromarray(img_array)\n",
    "        if not os.path.exists('poliface_run_max'):\n",
    "            os.mkdir('poliface_run_max')\n",
    "        if not os.path.exists('poliface_run_max/'):\n",
    "            os.mkdir('poliface_run_max/')\n",
    "        im2 = im2.save(\"poliface_run_max/{}.jpg\".format(count))\n",
    "        count+=1\n",
    "        #img_name=\"poliface_run/{}/{}.jpg\".format(name,count)\n",
    "        #cv2.imwrite(img_name,temp_frame)\n",
    "        #print(\" saved: {}\".format(count))\n",
    "        if min_dist<0.60:\n",
    "            #temp_frame = cv2.putText(temp_frame, name+' '+str(min_dist), c1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "            print(name+' '+str(min_dist)+' index'+str(count) )\n",
    "            count+=1\n",
    "            #print(temp_frame)\n",
    "            name_test.append(name)\n",
    "            name_true.append(temp_name)\n",
    "                    \n",
    "print(len(name_true),len(name_test)) \n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy rate:')\n",
    "print(accuracy_score(name_true, name_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ds = datasets.ImageFolder(\\'poliface_test\\')\\nidx_to_class_test={i:c for c,i in ds.class_to_idx.items()}\\ndef collate_fn(x):\\n    return x[0]\\nloader_test = DataLoader(ds,collate_fn=collate_fn)\\nname_list_test=[]\\nembedding_list_test=[]\\nfor img,idx in loader_test:\\n    face,prob = mtcnn0(img,return_prob= True)\\n    if face is not None and prob>0.92:\\n        emb_test=resnet(face.unsqueeze(0))\\n        embedding_list_test.append(emb_test.detach())\\n        name_list_test.append(idx_to_class_test[idx])\\ndata_test=[embedding_list_test,name_list_test]\\ntorch.save(data_test,\\'data_test.pt\\')\\n#print(embedding_list_test[1])\\nembedding_list = np.array(embedding_list)\\nembedding_list_test = np.array(embedding_list_test)\\nprint(embedding_list.shape, embedding_list_test.shape)\\nprint(len(name_list), len(name_list_test))\\nprint(\"a\")\\n#print(embedding_list[1])\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n\\ndef _most_similarity(embed_vecs, vec, labels):\\n  sim = cosine_similarity(embed_vecs, vec)\\n  sim = np.squeeze(sim, axis = 1)\\n  argmax = np.argsort(sim)[::-1][:1]\\n  label = [labels[idx] for idx in argmax][0]\\n  print(label)\\n  return label\\ny_preds=[]\\nprobs = [t.detach().numpy() for t in embedding_list]\\nprint(probs[1])\\nprint(probs[0][1])\\nfor i in range(len(embedding_list_test)):\\n  vec = embedding_list_test[i].reshape(1,-1)\\n  y_preds=_most_similarity(probs,vec,name_list)\\n#from sklearn.metrics import accuracy_score\\n#print(accuracy_score)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing \n",
    "\"\"\"ds = datasets.ImageFolder('poliface_test')\n",
    "idx_to_class_test={i:c for c,i in ds.class_to_idx.items()}\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "loader_test = DataLoader(ds,collate_fn=collate_fn)\n",
    "name_list_test=[]\n",
    "embedding_list_test=[]\n",
    "for img,idx in loader_test:\n",
    "    face,prob = mtcnn0(img,return_prob= True)\n",
    "    if face is not None and prob>0.92:\n",
    "        emb_test=resnet(face.unsqueeze(0))\n",
    "        embedding_list_test.append(emb_test.detach())\n",
    "        name_list_test.append(idx_to_class_test[idx])\n",
    "data_test=[embedding_list_test,name_list_test]\n",
    "torch.save(data_test,'data_test.pt')\n",
    "#print(embedding_list_test[1])\n",
    "embedding_list = np.array(embedding_list)\n",
    "embedding_list_test = np.array(embedding_list_test)\n",
    "print(embedding_list.shape, embedding_list_test.shape)\n",
    "print(len(name_list), len(name_list_test))\n",
    "print(\"a\")\n",
    "#print(embedding_list[1])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _most_similarity(embed_vecs, vec, labels):\n",
    "  sim = cosine_similarity(embed_vecs, vec)\n",
    "  sim = np.squeeze(sim, axis = 1)\n",
    "  argmax = np.argsort(sim)[::-1][:1]\n",
    "  label = [labels[idx] for idx in argmax][0]\n",
    "  print(label)\n",
    "  return label\n",
    "y_preds=[]\n",
    "probs = [t.detach().numpy() for t in embedding_list]\n",
    "print(probs[1])\n",
    "print(probs[0][1])\n",
    "for i in range(len(embedding_list_test)):\n",
    "  vec = embedding_list_test[i].reshape(1,-1)\n",
    "  y_preds=_most_similarity(probs,vec,name_list)\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#print(accuracy_score)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while True:\\n    ret, frame = cam.read()\\n    if not ret:\\n        print(\"fail to grab frame, try again\")\\n        break\\n        \\n    img = Image.fromarray(frame)\\n    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \\n    \\n    if img_cropped_list is not None:\\n        boxes, _ = mtcnn.detect(img)\\n                \\n        for i, prob in enumerate(prob_list):\\n            if prob>0.90:\\n                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \\n                \\n                dist_list = [] # list of matched distances, minimum distance is used to identify the person\\n                \\n                for idx, emb_db in enumerate(embedding_list):\\n                    dist = torch.dist(emb, emb_db).item()\\n                    dist_list.append(dist)\\n\\n                min_dist = min(dist_list) # get minumum dist value\\n                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\\n                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\\n                \\n                box = boxes[i] \\n                \\n                original_frame = frame.copy() # storing copy of frame before drawing on it\\n                c1,c2 = (int(np.float32(box[0])),int(np.float32(box[1]))),(int(np.float32(box[2])),int(np.float32(box[3])))\\n                if min_dist<0.90:\\n                    frame = cv2.putText(frame, name+\\' \\'+str(min_dist), c1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\\n                    print(name+\\' \\'+str(min_dist))\\n                    print(frame)\\n                    frame = cv2.rectangle(frame, c1 , c2, (255,0,0), 2)\\n                    \\n\\n    cv2.imshow(\"IMG\", frame)\\n        \\n    \\n    k = cv2.waitKey(1)\\n    if k%256==27: # ESC\\n        print(\\'Esc pressed, closing...\\')\\n        break\\n        \\n    elif k%256==32: # space to save image\\n        print(\\'Enter your name :\\')\\n        name = input()\\n        \\n        # create directory if not exists\\n        if not os.path.exists(\\'photos/\\'+name):\\n            os.mkdir(\\'photos/\\'+name)\\n            \\n        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\\n        cv2.imwrite(img_name, original_frame)\\n        print(\" saved: {}\".format(img_name))\\n        \\n        \\ncam.release()\\ncv2.destroyAllWindows()#\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using webcam recognize face\n",
    "\n",
    "# loading data.pt file\n",
    "#load_data = torch.load('data.pt') \n",
    "#embedding_list = load_data[0] \n",
    "#name_list = load_data[1] \n",
    "\n",
    "#cam = cv2.VideoCapture(0) \n",
    "\n",
    "\"\"\"while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"fail to grab frame, try again\")\n",
    "        break\n",
    "        \n",
    "    img = Image.fromarray(frame)\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "                \n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "                c1,c2 = (int(np.float32(box[0])),int(np.float32(box[1]))),(int(np.float32(box[2])),int(np.float32(box[3])))\n",
    "                if min_dist<0.90:\n",
    "                    frame = cv2.putText(frame, name+' '+str(min_dist), c1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                    print(name+' '+str(min_dist))\n",
    "                    print(frame)\n",
    "                    frame = cv2.rectangle(frame, c1 , c2, (255,0,0), 2)\n",
    "                    \n",
    "\n",
    "    cv2.imshow(\"IMG\", frame)\n",
    "        \n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256==27: # ESC\n",
    "        print('Esc pressed, closing...')\n",
    "        break\n",
    "        \n",
    "    elif k%256==32: # space to save image\n",
    "        print('Enter your name :')\n",
    "        name = input()\n",
    "        \n",
    "        # create directory if not exists\n",
    "        if not os.path.exists('photos/'+name):\n",
    "            os.mkdir('photos/'+name)\n",
    "            \n",
    "        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\n",
    "        cv2.imwrite(img_name, original_frame)\n",
    "        print(\" saved: {}\".format(img_name))\n",
    "        \n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()#\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "21e8149a302f809f6e44715469d2988c3f7ecd5e98094d047ad951395d4a18c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
